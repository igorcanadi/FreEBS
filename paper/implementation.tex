\subsection{Reads and Writes}
\label{sec:readwrite}
There are two operations that are issued by the driver --- read and write.
The driver keeps track of the most recent version for each replica in a list.
This structure is updated on a write and used to decide which replica to 
request a read from.

Figure~\ref{fig:write} shows the message flow on a write request. Notice
that we utilize chained replication, where the closer the replica is to the 
head of the chain the more more up-to-date it is. The choice for this chained
scheme is to reduce the network load to the driver. Thus, when 
the driver sends a write request, \texttt{sdaemon} issues the write to its 
local LSVD copy and then propagates the request to the next replica in the 
chain. After writing to its own LSVD, each replica sends back a response to
the driver with a status and its current version number. The driver then 
updates its replica list with the version in the sent response. If a majority
of replicas responds to the driver with a SUCCESS message, then the driver 
reports a success back to the kernel. Otherwise, it will report a failure. 

In contrast, a read only needs one replica to respond for a success. In 
Figure~\ref{fig:read} we observe that when the driver issues a read request 
it issues the request to the replica with the most recent version. Typically 
this is the primary replica. If the replica responds with a SUCCESS, then 
the driver serves the request back to the kernel. Otherwise, it will issue 
the request to the next replica in the chain with the most recent version. I 
no replicas respond, then the driver reports a failure.

\begin{figure}[t]
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth, trim=0 0 0 3.5in, clip]{./figures/write.pdf}
        \caption{The message flow for a write operation. A quorum of SUCCESS 
                 responses must be received for a successful write.}
        \label{fig:write}
    \end{subfigure}
    ~
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth, trim=0 0 0 3.5in, clip]{./figures/read.pdf}
        \caption{The message flow for a read operation. A read request is 
            issued to the replica with the most recent version until a 
            SUCCESS is received or if no replicas with the most recent 
            version responds with SUCCESS.}
        \label{fig:read}
    \end{subfigure}
    \caption{Message flows for read and write operations}
\end{figure}

\subsection{Replication}
\label{sec:replication}
Replication is managed by \texttt{sdaemon} using write propagation and 
synchronization. Propagation was discussed in Section~\ref{sec:readwrite}. 
Synchronization is performed whenever a replica joins the chain and also 
periodically to keep replicas near the tail from becoming increasingly out 
of date. Figure~\ref{fig:sync} shows how replication is performed. As we see,
replica C sends a SYNC request with the version number of its local LSVD 
copy to the previous replica in the chain, replica B. Replica B then responds
with all the writes from C.version to B.version. Replica C then applies all 
the writes to its LSVD. After the synchronization operation, Replica C's 
LSVD version is equal to B's version. Similarly, replica B initiates the 
synchronization procedure with replica A.

To handle failure, a replica controller is used to inform the driver and 
neighboring replicas that a failure has occured. This controller keeps track 
of the liveliness of each replica by recording the elapsed time since the 
last heartbeat message, which is sent every $T$ seconds by each replica. After
$2T$ seconds the controller flags the replica as \emph{inactive} and after 
$4T$ seconds the replica is marked as failed. Upon failure, the controller 
informs the kernel and the neighboring replicas using update messages. Upon
receipt of the update messages, the two neighboring replicas connect to each
other. The latter replica sends a SYNC request, and then normal operation 
resumes. Meanwhile, the controller spins off a new \texttt{sdaemon} process
that is appended to the end of the chain. Note that currently the replica 
controller is not implemented.

\begin{figure}[t]
    \includegraphics[width=0.45\textwidth, trim=0 2in 0 3in, clip]{./figures/sync.pdf}
    \caption{The message flow for a synchronization operation. Replica C 
            sends a SYNC request to its predecessor, which responds with all
            the writes since C's version.}
    \label{fig:sync}
\end{figure}

\subsection{Log Structured Virtual Disk}
\label{sec:lsvd}

When designing our file format to back virtual disk, we had few goals in mind. First, the file should be dynamically growing. If a disk was defined with $D$ bytes free space and user is using only $C$ bytes for his content, the backing file size should use $O(C)$ bytes. Second, we have to support versioning. We define version as update sequence number. Each update increments disk version by one. Update can consist of one or more sequential sectors on the disk and it's originating from Virtual Block Device in the kernel. To enable synchronization, storage daemon has to be aware of the current local disk version and has to be able to send arbitrary version updates to other replicas for synchronization. Third goals was consistency. If the local data has version $V$, it should include all updates with versions less than or equal to $V$ and none of the updates with versions bigger than $V$. Finally, our fourth goal was ability to easily support snapshots and incremental backups.

To address all these requirements, we developed Log Structured Virtual Disk (LSVD). When we receive updates, we just write them sequentially on a disk and increment version number by one. That way, we are dynamically growing. We also support versioning and are consistent, since we remember all updates together with their version numbers. When implementing the file format, we took special care to make sure that if we fail at any arbitrary point in time, the file is still consistent and can be recovered up to some recent version number. Finally, it is easy to support snapshoting and incremental backups. On snapshot operation, we just create new file and use old file as a base. New updates go to the new file and we can read old data from old, read-only base file. \XXX{Extend this?}

Figure \ref{fig:lsvd} shows the actual data layout in LSVD file. First few bytes define the superblock, which contains important metadata. Next, there are two checkpointing placeholders. After that, actual updates are layed out. Each update consists of data block and commit block. Data block can hold one or more sectors. Io our implementation, we use 4KB sector sizes. Commit block verifies integrity of the data block by storing its checksum. To be able to guarantee consistency, we need to have atomic version writes. If a commit block is not written to disk or checksum is invalid, we define the update as not committed.

\begin{figure}[h]
    \includegraphics[width=0.45\textwidth]{./figures/lsvd.pdf}
    \caption{The LSVD file structure.}
    \label{fig:lsvd}
\end{figure}


On a write, we just append write to the end of the file with additional metadata. Therefore, on a read we need to be able to find requested sectors on the disk. For that, we keep in-memory \emph{sector to offset map}. It maps disk sectors to file offsets where the sector's data is stored. On every write, we easily update the map. When opening a disk after recovery, we are able to rebuild sector to offset map by reading all the updates from the beginning of time. That would, however, be very slow. To keep recovery time from growing linearly with time, we do \emph{checkpointing}. Every 60 seconds, we write the sector to offset map to disk into one of the checkpointing placeholders at the beginning of the file. We execute checkpointing writes slowly. That way, we do not slow down regular write or read operations, especially since checkpointing writes are random writes. Superblock keeps a pointer to an active checkpoint. After the checkpoint is fully written to one of the placeholders, we update the superblock and \texttt{fsync}. During recovery, checkpoint is loaded into memory and recovery continues from the point in time when checkpoint was started. That way, we limit recovery time and keep it from not growing linearly with time.

The problem with storing data in log structured format is garbage collection. Even though a sector has been overwritten by newer data, we still keep an old version around. We can use it to our advantage and provide a feature that enables users to recover the disk state from arbitrary point in time. However, we decided to implement garbage collection with a goal of reclaiming state from sectors that have been overwritten with newer data. To do this, we developed a separate \texttt{cleanup} function that reads in a LSVD file and creates a new file containing only up-to-date updates. The \texttt{cleanup} function reads in the file and reconstructs sector to offset map. It then starts at the beginning of the file and considers all updates on the disk. If any of the sectors contained in an update is up-to-date, the update is appended to the new file. If all of the sectors have been overwritten, the update is skipped. Since the LSVD file is consistent on a disk at any point in time, we can execute cleanup during live operation. Once the new file is constructed, it can be brought up to date with the replica synchronization protocol. After it has been brought up to date, we can transparently change the active to and delete the old file. We have not implemented the hot swap, though. We evaluated cleanup function on offline LSVD files. There are two drawback of this garbage collection approach. First, it has to read in and process the whole file, which is a slow operation. Second, it requires enough free space on the current drive to store the new file. However, if we store $O(n)$ files on the disk and run cleanup on them in round robin fashion, we need free space for only $O(1)$ disks.

We implemented LSVD as a C library. Figure \ref{fig:lsvdinterface} shows the interface exposed by the library. Most of the functions are straight-forward. We use functions \texttt{get\_writes\_lsvd} and \texttt{put\_writes\_lsvd} to support replica synchronization. \texttt{get\_writes\_lsvd} returns raw update data for all updates since \texttt{first\_version}. We don't keep track of version to offset map, so we have to scan all the updates backwards until we encounter version less then or equal to \texttt{first\_version}. Once we find the requested starting version, we copy raw update data into the buffer. The raw data is then transferred to the lagging replica and appended to its LSVD file using function \texttt{put\_writes\_lsvd}, bringing it up to date.

\lstset{language=C}
\lstset{basicstyle=\small}
\lstset{frame=tlrb}
\begin{figure}
\begin{lstlisting}
struct lsvd_disk *create_lsvd(
	const char *pathname, uint64_t size);
struct lsvd_disk *open_lsvd(
	const char *pathname);
int cleanup_lsvd(const char *old_pathname, 
	const char *new_pathname);
int read_lsvd(struct lsvd_disk *lsvd,
	char *buf, uint64_t length,
	uint64_t offset, uint64_t version);
int write_lsvd(struct lsvd_disk *lsvd,
	const char *buf, uint64_t length, 
	uint64_t offset, uint64_t version);
int fsync_lsvd(struct lsvd_disk *lsvd);
uint64_t get_version(struct lsvd_disk *lsvd);
char *get_writes_lsvd(struct lsvd_disk *lsvd, 
	uint64_t first_version, size_t *size);
int put_writes_lsvd(struct lsvd_disk *lsvd, 
	uint64_t first_version, char *buf,
	size_t size);
int close_lsvd(struct lsvd_disk *lsvd);
\end{lstlisting}
\caption{LSVD library interface}
\label{fig:lsvdinterface}
\end{figure}
