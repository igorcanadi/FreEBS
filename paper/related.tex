\subsection{DRBD}
\label{sec:drbd}
DRBD\textsuperscript{\textregistered} is a virtual RAID-1 block storage 
device consisting of two shared-nothing node that form a highly available
\emph{cluster}. The DRBD\textsuperscript{\textregistered} interface is 
implemented as a kernel module that can be mounted like a normal block 
device. Since it is an open-source project, many aspects of its 
implementation are availble; however, in this section we focus on the
details of replication and synchronization.

\subsubsection{Replication}
DRBD\textsuperscript{\textregistered} uses a RAID-1 method of replication, 
meaning there are two nodes in a basic DRBD\textsuperscript{\textregistered} 
cluster --- one \emph{primary} and one \emph{secondary} --- that each
contain typical Linux kernel components\cite{drbd, drbd_manual}. The 
primary node 
services all reads and writes, and the secondary node fully mirrors the 
primary node by propagation of writes across the network. If the primary 
node fails, then service migrates to the secondary node. There are two 
main modes for replication --- fully synchronous and asynchronous. 
The former means that the primary node reports a completed write only after 
the write has been committed to both nodes in the cluster. The latter means 
that each node reports a successful write as soon as the data is written to 
its local disk. 

FreEBS uses a similar technique to the asynchronous method of replication;
we utilize a primary replica that services reads and propagates write 
requests. However, FreEBS waits for a majority of replicas to respond with a
successful write completion. Our system also offers much more flexibility in 
terms of the number of configurable replicas. For instance, 
DRBD\textsuperscript{\textregistered} must use 
stacked DRBD volumes in order to create more than two replicas, which can 
get cumbersome as we add more replicas.

\subsubsection{Synchronization} 
DRBD\textsuperscript{\textregistered} offers 
variable-rate and fixed-rate synchronization\cite{drbd_manual}.
For the first method, DRBD\textsuperscript{\textregistered} selects a 
synchronization rate
based on the network bandwidth. For the fixed-rate case, synchronization is
performed periodically at some constant time interval. Synchronization 
can be made more efficient by using checksums to identify blocks that have 
changed since the last synchronization. This eliminates the need to 
synchronize blocks that were overwritten with identical contents. Changes
are tracked using the activity log (AL) which records \emph{hot extents}, 
the blocks that have been modified between synchronization points. The 
activity log uses a quick-sync bitmap to keep track of modified blocks.

FreEBS uses a fixed-rate synchronization scheme that allows nodes to request 
all writes since a particular version. This is done by backward traversal 
through our log-based backing file until the requested version is found, 
eliminating the need to have a separate activity log.

%\emph{Should we include the DRBD figure here?}

\subsection{Cloud Storage}
Rackspace Cloud Block Storage (CBS) and HP Cloud Block Storage provide 
persistent storage for their cloud instances\cite{rackspace, hp}. Both also 
support snapshots. However, unlike EBS snapshots, Rackspace CBS contains the 
full directory structure of a volume. Both HP CBS and Rackspace CBS are 
built on the OpenStack storage platform. Another commercially available 
cloud block storage is Zadara\texttrademark Virtual Private Storage 
Arrays\cite{zadara}, which is a cloud-based NAS service that iSCSI and NFS 
protocols.  

\subsection{Network Block Level Storage Systems}
There is a fair amount of work on the subject of network block level storage.
For instance, in 1996 Lee and Thekkath published a paper on Petal, a 
distributed virtual disk system\cite{lee1996petal}. In this system, a cluster
of servers manage a shared pool of physical disks and present the client 
with a virtual disk. Virtual disks are globally accessible to all
Petal clients, not one-to-one like FreEBS. Petal handles failure by 
chain-replicating blocks with one neighboring server. If one disk goes down,
then the server with the replica block will serve the request. 

In \cite{dabek2001wide} Dabek et al. discuss Cooperative File System (CFS), 
a peer-to-peer read-only block storage system with robustness and 
scalability in mind. CFS uses a distributed hash table to map blocks to a CFS
server and utilizes block-level replication to increase reliability. 
Block replicas exist on the subsequent $k$ servers based on the corresponding
hash value for that block; thus, the next server in the hash takes up 
responsibility for serving that block.  

Conversely, Quinlan and Dorward present Venti, a network storage system that
uses a write-once only policy for archival data\cite{quinlan2002venti}. Venti
is a userspace daemon that performs block level reads and permanent writes. 
Venti provides reliability and recovery by relying on RAID-5 disk arrays 
rather than mirroring.  

The main difference between these works and FreEBS is that they are built 
for a shared virtual disk system and thus consider scalability and load
balancing. FreEBS volumes, by contrast, only support being attached to one 
machine at a time.

